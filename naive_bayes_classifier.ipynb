{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n",
      "({'TP': 50, 'TN': 36, 'FP': 1, 'FN': 13}, {'precision_ham': 0.9803921568627451, 'precision_spam': 0.7346938775510204, 'recall_ham': 0.7936507936507936, 'recall_spam': 0.972972972972973, 'f_score_ham': 0.8771929824561403, 'f_score_spam': 0.8372093023255813})\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "\n",
    "ALPHA = 0.05\n",
    "V = 200000\n",
    "\n",
    "\"\"\"\n",
    "Counts the number of words in each email and returns a dictionary of word counts as well as the total count of words.\n",
    "\"\"\"\n",
    "def count_words(directory):\n",
    "    words = {}\n",
    "    count = 0\n",
    "    for filename in glob.glob(directory):\n",
    "        f = open(filename)\n",
    "        for line in f.readlines():\n",
    "            line = line.strip(\"\\n\")\n",
    "            if line not in words:\n",
    "                words[line] = 1\n",
    "            else:\n",
    "                words[line] += 1\n",
    "            count += 1\n",
    "    return words, count\n",
    "\n",
    "\"\"\"\n",
    "Runs the naive bayes model by adding to the total probability of ham and spam, respectively.\n",
    "Returns a dictionary containing a mapping of the file name to the model classification and the truth.\n",
    "Also returns the accuracy of the model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def run_model(directory, spam, prob_spam, prob_unseen_spam, ham, prob_ham, \n",
    "                prob_unseen_ham, truth_table):\n",
    "    classification = {}\n",
    "    differences = 0\n",
    "    total_email = 0\n",
    "\n",
    "    for email in glob.glob(directory):\n",
    "        total_prob_ham = 0\n",
    "        total_prob_spam = 0\n",
    "        f = open(email)\n",
    "\n",
    "        for word in f.readlines():\n",
    "            word = word.strip(\"\\n\")\n",
    "            if word not in ham:\n",
    "                total_prob_ham += prob_unseen_ham\n",
    "            else:\n",
    "                total_prob_ham += ham[word]\n",
    "\n",
    "            if word not in spam:\n",
    "                total_prob_spam += prob_unseen_spam\n",
    "            else:\n",
    "                total_prob_spam += spam[word]\n",
    "\n",
    "        email = email.split(\"/\")\n",
    "        email = email[len(email) - 1].strip(\".words\")\n",
    "\n",
    "        if email in truth_table:\n",
    "            truth = \"Spam\"\n",
    "        else:\n",
    "            truth = \"Ham\"\n",
    "\n",
    "        if total_prob_ham > total_prob_spam:\n",
    "            generated = \"Ham\"\n",
    "            classification[email] = {\"classification\": generated, \"truth\": truth}\n",
    "        else:\n",
    "            generated = \"Spam\"\n",
    "            classification[email] = {\"classification\": generated, \"truth\": truth}\n",
    "        \n",
    "        if generated != truth:\n",
    "            differences += 1\n",
    "        total_email += 1\n",
    "\n",
    "    return classification, 1 - (differences/total_email)\n",
    "\n",
    "\"\"\"\n",
    "Extracts the names of the files that should be \"Spam\". \n",
    "Returns a set with the files that should be classified as \"Spam.\"\n",
    "\"\"\"\n",
    "def populate_truth(directory):\n",
    "    truth = set()\n",
    "\n",
    "    for filename in glob.glob(directory):\n",
    "        f = open(filename)\n",
    "        for line in f.readlines():\n",
    "            line = line.strip(\"\\n\")\n",
    "            truth.add(line)\n",
    "    return truth\n",
    "\n",
    "\"\"\"\n",
    "Calculates the probabilities necessary to implement the Naive Bayes classifier.\n",
    "Adds the P(word|class) to the dictionary of words and returns P(class) and P(unseen_word) \n",
    "\"\"\"\n",
    "def calculate_probabilities(total_words, total_class, words, is_ham=False):\n",
    "    for word in words:\n",
    "        curr_count = words[word] + ALPHA\n",
    "        log_prob = math.log(curr_count/(total_class + (V * ALPHA)))\n",
    "        words[word] = log_prob\n",
    "    \n",
    "    prob_class = math.log(total_class/total_words)\n",
    "    prob_unseen_class = ALPHA/(total_class +(V*ALPHA))\n",
    "    prob_unseen_class = math.log(prob_unseen_class)\n",
    "\n",
    "    return prob_class, prob_unseen_class\n",
    "\n",
    "\"\"\"\n",
    "After the classifier is run, obtains the metrics of precision, recall, and f-score as well as the confusion matrix.\n",
    "Returns confusion matrix and metrics as two separate dictionaries.\n",
    "\"\"\"\n",
    "def get_metrics(results):\n",
    "    \"Assuming Ham is positive and Spam is negative\"\n",
    "    positive = \"Ham\"\n",
    "    negative = \"Spam\"\n",
    "    confusion_matrix = {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0}\n",
    "\n",
    "    for val in results.values():\n",
    "        if val[\"truth\"] == positive:\n",
    "            if val[\"classification\"] == positive:\n",
    "                confusion_matrix[\"TP\"] += 1\n",
    "            else:\n",
    "                confusion_matrix[\"FN\"] += 1\n",
    "\n",
    "        else:\n",
    "            if val[\"classification\"] == negative:\n",
    "                confusion_matrix[\"TN\"] += 1\n",
    "            else:\n",
    "                confusion_matrix[\"FP\"] += 1\n",
    "    metrics = {}\n",
    "\n",
    "    metrics[\"precision_ham\"] = confusion_matrix[\"TP\"] / (confusion_matrix[\"TP\"] + confusion_matrix[\"FP\"])\n",
    "    metrics[\"precision_spam\"] = confusion_matrix[\"TN\"] / (confusion_matrix[\"TN\"] + confusion_matrix[\"FN\"])\n",
    "    metrics[\"recall_ham\"] = confusion_matrix[\"TP\"] / (confusion_matrix[\"TP\"] + confusion_matrix[\"FN\"])\n",
    "    metrics[\"recall_spam\"] = confusion_matrix[\"TN\"] / (confusion_matrix[\"TN\"] + confusion_matrix[\"FP\"])\n",
    "\n",
    "    metrics[\"f_score_ham\"] = \\\n",
    "        (2 * metrics[\"recall_ham\"] * metrics[\"precision_ham\"]) / (metrics[\"precision_ham\"] + metrics[\"recall_ham\"])\n",
    "    metrics[\"f_score_spam\"] = \\\n",
    "        (2 * metrics[\"recall_spam\"] * metrics[\"precision_spam\"]) / (metrics[\"precision_spam\"] + metrics[\"recall_spam\"])\n",
    "  \n",
    "    return confusion_matrix, metrics\n",
    "\n",
    "def main():\n",
    "    ham = \"./data/ham/*\"\n",
    "    spam = \"./data/spam/*\"\n",
    "    test = \"./data/test/*\"\n",
    "    truth_file = \"./data/truthfile*\"\n",
    "\n",
    "    dict_ham, total_ham = count_words(ham)\n",
    "    dict_spam, total_spam = count_words(spam)\n",
    "    total_words = total_ham + total_spam\n",
    "\n",
    "    prob_ham, prob_unseen_ham = calculate_probabilities(total_words, total_ham, dict_ham, True)\n",
    "    prob_spam, prob_unseen_spam = calculate_probabilities(total_words, total_spam, dict_spam)\n",
    "\n",
    "    truth_table = populate_truth(truth_file)\n",
    "\n",
    "    results, accuracy = run_model(test, dict_spam, prob_spam, prob_unseen_spam, \n",
    "                                    dict_ham, prob_ham, prob_unseen_ham, truth_table)\n",
    "    \n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    print(get_metrics(results))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
